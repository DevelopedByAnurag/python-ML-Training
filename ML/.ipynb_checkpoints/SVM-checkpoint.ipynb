{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM stands for a support vector machine. SVM's are typically used for classification tasks similar to what we did with K Nearest Neighbors. They work very well for high dimensional data and are allow for us to classify data that does not have a linear correspondence. For example classifying a data set like the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm-data-768x578.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to use K Nearest Neighbors to do this would likely give us a very low accuracy score and is not favorable. This is where SVM's are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start we need to import a few things from sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials we did quite a bit of work to load in our data sets from places like the UCI Machine Learning Repository. That is a very useful skill and is something you will often have to do when applying these algorithm to your own data. However, now that we have learned this we will use the data sets that come with sklearn. These are much nicer to work with and have some nice methods that make loading in data very quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will be using a breast cancer data set. It consists of many features describing a tumor and classifies them as either cancerous or non cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load our data we will simply do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of the features in the data set we can do:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(\"Features: \", cancer.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded in our data set it is time to split it into training and testing data. We will do this like seen in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "x = cancer.data  # All of the features\n",
    "y = cancer.target  # All of the labels\n",
    "print(type(x))\n",
    "print(type(y))\n",
    "print(x)\n",
    "print(y)\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to have a look at our data we can print the first few instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.051e+01 2.781e+01 1.344e+02 1.319e+03 9.159e-02 1.074e-01 1.554e-01\n",
      "  8.340e-02 1.448e-01 5.592e-02 5.240e-01 1.189e+00 3.767e+00 7.001e+01\n",
      "  5.020e-03 2.062e-02 3.457e-02 1.091e-02 1.298e-02 2.887e-03 2.447e+01\n",
      "  3.738e+01 1.627e+02 1.872e+03 1.223e-01 2.761e-01 4.146e-01 1.563e-01\n",
      "  2.437e-01 8.328e-02]\n",
      " [1.349e+01 2.230e+01 8.691e+01 5.610e+02 8.752e-02 7.698e-02 4.751e-02\n",
      "  3.384e-02 1.809e-01 5.718e-02 2.338e-01 1.353e+00 1.735e+00 2.020e+01\n",
      "  4.455e-03 1.382e-02 2.095e-02 1.184e-02 1.641e-02 1.956e-03 1.515e+01\n",
      "  3.182e+01 9.900e+01 6.988e+02 1.162e-01 1.711e-01 2.282e-01 1.282e-01\n",
      "  2.871e-01 6.917e-02]\n",
      " [1.359e+01 1.784e+01 8.624e+01 5.723e+02 7.948e-02 4.052e-02 1.997e-02\n",
      "  1.238e-02 1.573e-01 5.520e-02 2.580e-01 1.166e+00 1.683e+00 2.222e+01\n",
      "  3.741e-03 5.274e-03 1.065e-02 5.044e-03 1.344e-02 1.126e-03 1.550e+01\n",
      "  2.610e+01 9.891e+01 7.391e+02 1.050e-01 7.622e-02 1.060e-01 5.185e-02\n",
      "  2.335e-01 6.263e-02]\n",
      " [1.189e+01 2.117e+01 7.639e+01 4.338e+02 9.773e-02 8.120e-02 2.555e-02\n",
      "  2.179e-02 2.019e-01 6.290e-02 2.747e-01 1.203e+00 1.930e+00 1.953e+01\n",
      "  9.895e-03 3.053e-02 1.630e-02 9.276e-03 2.258e-02 2.272e-03 1.305e+01\n",
      "  2.721e+01 8.509e+01 5.229e+02 1.426e-01 2.187e-01 1.164e-01 8.263e-02\n",
      "  3.075e-01 7.351e-02]\n",
      " [1.328e+01 1.372e+01 8.579e+01 5.418e+02 8.363e-02 8.575e-02 5.077e-02\n",
      "  2.864e-02 1.617e-01 5.594e-02 1.833e-01 5.308e-01 1.592e+00 1.526e+01\n",
      "  4.271e-03 2.073e-02 2.828e-02 8.468e-03 1.461e-02 2.613e-03 1.424e+01\n",
      "  1.737e+01 9.659e+01 6.237e+02 1.166e-01 2.685e-01 2.866e-01 9.173e-02\n",
      "  2.736e-01 7.320e-02]] [0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5], y_train[:5])\n",
    "classes= ['malignant' 'benign']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tutorial will explain how a SVM works and the math behind it. Following that I will go over implementing the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What a SVM Does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM has a large list of applicable uses. However, in machine learning it is typically used for classification. It is a powerful tool that is a good choice for classifying complicated data with a high degree of dimensions(features). Note that K-Nearest Neighbors does not perform well on high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How A Support Vector Machine Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short a support vector machine works by dividing data into multiple classes using something called a hyper-plane. A hyper plane is a fancy word for something that is straight that can divide data points. In 2D space a hyper-plane is simply a line, in 3D space a hyper-plane is a plane. In any space higher than 3D it is simply called a hyper-plane.\n",
    "\n",
    "Here’s an example of a hyper-plane for the data points on the 2D graph below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Planes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a hyper-plane we need to do the following. We must pick two points that are known as our support vectors. These points must be the two closest points to the hyper-plane and their distance from the hyper-plane must be identical. In the example above we can see that the two circled points are our support vectors and their distance to the hyper-plane is the same, they are also the closest points. With this rule we can actually create an infinite amount of hyper planes (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the images above are valid hyper-planes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking a Hyper Plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create a hyper-plane we are going to use it to classify our data. If a test point is on the left side of the plane we would classify it as red (in our examples above) and if it is on the right we would classify it as green. So how can we pick a hyper-plane that will give us the best classification predictions?\n",
    "\n",
    "Have a look at the hyper-planes above and determine which you think would give the best classification for a mystery test point. What do you notice about that hyper-plane?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the best possible hyper-plane would be the first image on this page. Notice the distance between the support vectors and the hyper-plane is far greater than the other generated hyper-planes.\n",
    "\n",
    "When we pick a hyper-plane we want to pick one that has the greatest possible margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The margin is the distance that separates all of the points in our test data. The blue lines below show you the margin for this particular data and hyper-plane. Typically the greater our margin the better our classification will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm5.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you now have a very basic understanding of how a SVM works. Seems pretty simple in theory, but in practice we can run into a lot of issues.\n",
    "\n",
    "Let’s say our data isn’t as pretty and we have some points that look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm6.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you determine which hyper-plane would be the best for this data? Even if you could it would make a horrible classifier. This is where we introduce something called kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels provide a way for us to create a hyper-plane for data like seen above. We use a kernel to bring our data up to a higher dimension (in this case from 2D->3D). We hope that by doing this we will have our points plotted in a way that we can divide them using a hyper-plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying a kernel to our data above we hope to get something that looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm8.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we can now divide our points with a plane in 3D. By applying the kernel our data has become separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is A Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kernel is simply a function that takes as input our features (x1, x2 in our example) and returns a value equal to the third dimensional coordinate (x3). An example of a kernel copuld be the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(x1)^2 + (x2)^2 = x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm7.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically when we use a kernel we use a pre-existing one. There is much debate about which kernel is the best but here are some examples of popular kernels."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "– Linear\n",
    "– Polynomial\n",
    "– Circular\n",
    "– Hyperbolic Tangent (Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft & Hard Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last topic to touch on is soft and hard margins. A hard margin is precisely what you’ve learned already, no points may exist inside the margin. However, sometimes if we have outlier points we want to allow them to exist inside the margin and use points that are not the closest to the hyper-plane to be our support vectors. Doing this is called using a soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://techwithtim.net/wp-content/uploads/2019/01/svm9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the example above that there is a point that exists inside the margin. If we had not allowed this not only would it be difficult to create a hyper-plane but our classification would perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of points you allow to exists inside the margin is something we can define as hyper-parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the SVM is actually fairly easy. We can simply create a new model and call .fit() on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score our data we will use a useful tool from the sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(x_test) # Predict values for our test data\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred) # Test them against our correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "[1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is all we need to do to implement our SVM, now we can run the program and take note of our amazing accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6403508771929824\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait... Our accuracy is close to 60% and that is horrible! Looks like we need to add something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Kernel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The reason we received such a low accuracy score was we forgot to add a kernel! We need to specify which kernel we should use to increase our accuracy.\n",
    "\n",
    "Kernel Options:\n",
    "- linear\n",
    "- poly\n",
    "- rbf\n",
    "- sigmoid\n",
    "- precomputed\n",
    " If none is given, ‘rbf’ will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use linear for this data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this we receive a much better accuracy of close to 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(x_test) # Predict values for our test data\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred) # Test them against our correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 1   Y_TEST: 1\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n",
      "Y_PRED : 0   Y_TEST: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    print(\"Y_PRED :\", y_pred[i],\" \",\"Y_TEST:\",y_test[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default our kernel has a soft margin of value 1. This parameter is known as C. We can increase C to give more of a soft margin, we can also decrease it to 0 to make a hard margin. Playing with this value should alter your results slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel=\"linear\", C=2)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(x_test) # Predict values for our test data\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred) # Test them against our correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play around with some other parameters have a look here.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to KNearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see how this algorithm runs in comparison to KNN we can run the KNN classifier on this data-set and compare our accuracy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change to the KNN classifier is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=11)\n",
    "# Simply change clf to what is above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that KNN still does well on this data set but hovers around the 90% mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
